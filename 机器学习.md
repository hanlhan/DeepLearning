# 1. 机器学习

## 1）哪些机器学习算法不需要做归一化处理？

在实际应用中，需要归一化的模型：
1.基于距离计算的模型：KNN。
2.通过梯度下降法求解的模型：线性回归、逻辑回归、支持向量机、神经网络。

但树形模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、随机森林(Random Forest)。

## 2）归一化和标准化

归一化：把每个特征向量（特别是奇异样本数据）的值都缩放到相同数值范围，如[0,1]或[-1,1]。

![img](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155058399446105850.png)

这个方法经常用于确保数据点没有因为特征的基本性质而产生较大差异，即确保数据处于同一数量级（同一量纲），提高不同特征数据的可比性。

标准化：使特征均值为0，方差为1

![img](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155058398881840727.png)

## 3、树形结构为什么不需要归一化？

因为数值缩放不影响分裂点位置，对树模型的结构不造成影响。
按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。而且，树模型是不能进行梯度下降的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，并且求导没意义，也就不需要归一化。

**既然树形结构（如决策树、RF）不需要归一化，那为何非树形结构比如Adaboost、SVM、LR、Knn、KMeans之类则需要归一化呢？**

对于线性模型，特征值差别很大时，比如说LR，我有两个特征，一个是(0,1)的，一个是(0,10000)的，运用梯度下降的时候，损失等高线是椭圆形，需要进行多次迭代才能到达最优点。
但是如果进行了归一化，那么等高线就是圆形的，促使SGD往原点迭代，从而导致需要的迭代次数较少。



##  4、在k-means或kNN，我们常用欧氏距离来计算最近的邻居之间的距离，有时也用曼哈顿距离，请对比下这两种距离的差别

欧氏距离虽然很有用，但也有明显的缺点。它将样本的不同属性（即各指标或各变量量纲）之间的差别等同看待，这一点有时不能满足实际要求。例如，在教育研究中，经常遇到对人的分析和判别，个体的不同属性对于区分个体有着不同的重要性。因此，欧氏距离适用于向量各分量的度量标准统一的情况。

##  6、请简要说说一个完整机器学习项目的流程

1 抽象成数学问题
明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。
这里的抽象成数学问题，指的我们明确我们可以获得什么样的数据，目标是一个分类还是回归或者是聚类的问题，如果都不是的话，如果划归为其中的某类问题。

2 获取数据
数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。
数据要有代表性，否则必然会过拟合。
而且对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。
而且还要对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。

3 特征预处理与特征选择
良好的数据要能够提取出良好的特征才能真正发挥效力。
特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。
筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。

4 训练模型与调优
直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。

5 模型诊断
如何确定模型调优的方向与思路呢？这就需要对模型进行诊断的技术。
过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。
误差分析 也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题……
诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。

6 模型融合
一般来说，模型融合后都能使得效果有一定提升。而且效果很好。
工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。

7 上线运行
这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。
这些工作流程主要是工程实践上总结出的一些经验。并不是每个项目都包含完整的一个流程。这里的部分只是一个指导性的说明，只有大家自己多实践，多积累项目经验，才会有自己更深刻的认识。

## 9、overfitting怎么解决

overfitting就是过拟合, 其直观的表现如下图所示，随着训练过程的进行，模型复杂度增加，在training data上的error渐渐减小，但是在验证集上的error却反而渐渐增大——因为训练出来的网络过拟合了训练集, 对训练集外的数据却不work, 这称之为泛化(generalization)性能不好。泛化性能是训练的效果评价中的首要目标，没有良好的泛化，就等于南辕北辙, 一切都是无用功。

实际训练中, 降低过拟合的办法一般如下：

正则化(Regularization)
L2正则化：目标函数中增加所有权重w参数的平方之和, 逼迫所有w尽可能趋向零但不为零. 因为过拟合的时候, 拟合函数需要顾忌每一个点, 最终形成的拟合函数波动很大, 在某些很小的区间里, 函数值的变化很剧烈, 也就是某些w非常大. 为此, L2正则化的加入就惩罚了权重变大的趋势.
L1正则化：目标函数中增加所有权重w参数的绝对值之和, 逼迫更多w为零(也就是变稀疏. L2因为其导数也趋0, 奔向零的速度不如L1给力了). 大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，xi的大部分元素（也就是特征）都是和最终的输出yi没有关系或者不提供任何信息的，在最小化目标函数的时候考虑xi这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的特征权重反而会被考虑，从而干扰了对正确yi的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些无用的特征，也就是把这些特征对应的权重置为0。

随机失活(dropout)
在训练的运行的时候，让神经元以超参数p的概率被激活(也就是1-p的概率被设置为0), 每个w因此随机参与, 使得任意w都不是不可或缺的, 效果类似于数量巨大的模型集成。

逐层归一化(batch normalization)
这个方法给每层的输出都做一次归一化(网络上相当于加了一个线性变换层), 使得下一层的输入接近高斯分布. 这个方法相当于下一层的w训练时避免了其输入以偏概全, 因而泛化效果非常好. 

提前终止(early stopping)
理论上可能的局部极小值数量随参数的数量呈指数增长, 到达某个精确的最小值是不良泛化的一个来源. 实践表明, 追求细粒度极小值具有较高的泛化误差。这是直观的，因为我们通常会希望我们的误差函数是平滑的, 精确的最小值处所见相应误差曲面具有高度不规则性, 而我们的泛化要求减少精确度去获得平滑最小值, 所以很多训练方法都提出了提前终止策略. 典型的方法是根据交叉叉验证提前终止: 若每次训练前, 将训练数据划分为若干份, 取一份为测试集, 其他为训练集, 每次训练完立即拿此次选中的测试集自测. 因为每份都有一次机会当测试集, 所以此方法称之为交叉验证. 交叉验证的错误率最小时可以认为泛化性能最好, 这时候训练错误率虽然还在继续下降, 但也得终止继续训练了.  



##  10、LR和SVM的联系与区别

相同点

①都是线性分类器。本质上都是求一个最佳分类超平面。
②都是监督学习算法。
③都是判别模型。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。常见的判别模型有：KNN、SVM、LR，常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。

不同点

1) 本质上是损失函数不同 

LR的损失函数是交叉熵： 

![img](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155283661858409749.png)

SVM的目标函数： 

![img](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155283665316207879.jpg)

逻辑回归基于概率理论，假设样本为正样本的概率可以用sigmoid函数（S型函数）来表示，然后通过极大似然估计的方法估计出参数的值。 
支持向量机基于几何间隔最大化原理，认为存在最大几何间隔的分类面为最优分类面。

2) 两个模型对数据和参数的敏感程度不同 
SVM考虑分类边界线附近的样本（决定分类超平面的样本）。在支持向量外添加或减少任何样本点对分类决策面没有任何影响； 
LR受所有数据点的影响。直接依赖数据分布，每个样本点都会影响决策面的结果。如果训练数据不同类别严重不平衡，则一般需要先对数据做平衡处理，让不同类别的样本尽量平衡。

3) SVM 基于距离分类，LR 基于概率分类。 
SVM依赖数据表达的距离测度，所以需要对数据先做 normalization；LR不受其影响。

4) 在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。 
SVM算法里，只有少数几个代表支持向量的样本参与分类决策计算，也就是只有少数几个样本需要参与核函数的计算。 
LR算法里，每个样本点都必须参与分类决策的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。尤其是数据量很大时，我们无法承受。所以，在具体应用时，LR很少运用核函数机制。

5) 在小规模数据集上，Linear SVM要略好于LR，但差别也不是特别大，而且Linear SVM的计算复杂度受数据量限制，对海量数据LR使用更加广泛。

6) SVM的损失函数就自带正则，而 LR 必须另外在损失函数之外添加正则项。

7）LR是参数模型，svm是非参数模型



## 参数模型与非参数模型

在统计学中，参数模型通常假设总体服从某个分布，这个分布可以由一些参数确定，如正态分布由均值和标准差确定，在此基础上构建的模型称为参数模型；可以通过有限个参数来确定一个模型，这样的方式就是“有参数模型”

非参数模型对于总体的分布不做任何假设或者说是数据分布假设自由，只知道其分布是存在的，所以就无法得到其分布的相关参数，只能通过非参数统计的方法进行推断。

对于目标函数形式不作过多的假设的算法称为非参数机器学习算法。通过不做假设，算法可以自由的从训练数据中学习任意形式的函数。

所以说，参数模型和非参数模型中的“参数”并不是模型中的参数，而是数据分布的参数。 

常见的参数机器学习模型有：
1、逻辑回归（logistic regression）
2、线性成分分析（linear regression）
3、感知机（perceptron）（假设分类超平面是wx+b=0）

参数机器学习算法有如下优点:
1、简洁：理论容易理解和解释结果。
2、快速：参数模型学习和训练的速度都很快。
3、数据更少：通常不需要大量的数据，在对数据的拟合不很好时表现也不错。

参数机器学习算法的局限性：
1、拘束：以指定的函数形式来指定学习方式。
2、有限的复杂度：通常只能应对简单的问题。
3、拟合度小：实际中通常无法和潜在的目标函数完全吻合，也就是容易出现欠拟合。

常见的非参数机器学习模型有：
1、决策树
2、朴素贝叶斯
3、支持向量机（SVM的例子中，SVM的参数α数目和样本数目相同，从定义看来，因为参数数目和样本规模相当，所以属于无参数模型。当然，SVM通过得到支撑向量的方式，只有若干样本的参数α不为0，从这个角度，SVM还属于“稀疏模型”，这又属于另外一码事了。）
4、神经网络

非参数机器学习算法的优势有：
1、可变性：可以拟合许多不同的函数形式。
2、模型强大：对于目标函数不做假设或者作出很小的假设。
3、表现良好：对于训练样本数据具有良好的拟合性。

非参数机器学习算法的局限性：
1、需要更多数据：对于拟合目标函数需要更多的训练数据。
2、速度慢：因为需要训练跟多的参数，所以训练过程通常比较慢。
3、过拟合：有较高的风险发生过拟合，对于预测的效果解释性不高。

![img](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64159549570728608258.png)



## 什么是熵

计算机中一般指  信息（熵），将其定义为离散随机事件的出现概率。一个系统越是有序，信息熵就越低；反之，一个系统越是混乱，信息熵就越高。所以说，信息熵可以被认为是系统有序化程度的一个度量。



## 什么是梯度下降法

要使用梯度下降法找到一个函数的局部极小值，必须向函数上当前点对应梯度（或者是近似梯度）的反方向的规定步长距离点进行迭代搜索。如果相反地向梯度正方向迭代进行搜索，则会接近函数的局部极大值点；这个过程则被称为梯度上升法。

什么是梯度？简单理解，在单变量的实值函数的情况，梯度就是导数，或者，对于一个线性函数，也就是线的斜率。

![img](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com//Public/Image/Question/1512806482_849.png)

![img](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com//Public/Image/Question/1512806489_613.png)

![img](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com//Public/Image/Question/1512806495_246.png)

![img](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com//Public/Image/Question/1512806576_426.png)



## LR与线性回归的区别与联系

LR工业上一般指Logistic Regression(逻辑回归)而不是Linear Regression(线性回归). LR在线性回归的实数范围输出值上施加sigmoid函数将值收敛到0~1范围, 其目标函数也因此从差平方和函数变为对数损失函数, 以提供最优化所需导数（sigmoid函数是softmax函数的二元特例, 其导数均为函数值的f*(1-f)形式）。请注意, LR往往是解决二元0/1分类问题的, 只是它和线性回归耦合太紧, 不自觉也冠了个回归的名字(马甲无处不在). 若要求多元分类,就要把sigmoid换成大名鼎鼎的softmax了。
个人感觉逻辑回归和线性回归首先都是广义的线性回归，
其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，
另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。
逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。



## 简单说下有监督学习和无监督学习的区别

有监督学习：对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。（LR,SVM,BP,RF,GBDT）
无监督学习：对未标记的样本进行训练学习，比发现这些样本中的结构知识。(KMeans,PCA)



## 谈谈判别式模型和生成式模型？

判别方法：由数据直接学习决策函数 Y = f（X），或者由条件分布概率 P（Y|X）作为预测模型，即判别模型。
生成方法：由数据学习联合概率密度分布函数 P（X,Y）,然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型。
由生成模型可以得到判别模型，但由判别模型得不到生成模型。
常见的判别模型有：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场
常见的生成模型有：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机



## 为什么朴素贝叶斯如此“朴素”？

因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。

朴素贝叶斯模型(Naive Bayesian Model)的朴素(Naive)的含义是"很简单很天真"地假设样本特征彼此独立. 这个假设现实中基本上不存在, 但特征相关性很小的实际情况还是很多的, 所以这个模型仍然能够工作得很好。



## KNN中的K如何选取的？

如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；
如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。
K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。
    在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。



## 简述随机森林

随机森林是由很多个决策树组成的，首先要建立Bootstrap数据集，即从原始的数据中有放回地随机选取一些，作为新的数据集，新数据集中会存在重复的数据，然后对每个数据集构造一个决策树，但是不是直接用所有的特征来建造决策树，而是对于每一步，都从中随机的选择一些特征，来构造决策树，这样我们就构建了多个决策树，组成随机森林，把数据输入各个决策树中，看一看每个决策树的判断结果，统计一下所有决策树的预测结果，Bagging整合结果，得到最终输出。

## 随机森林如何处理缺失值？

根据随机森林创建和训练的特点，随机森林对缺失值的处理还是比较特殊的。
首先，给缺失值预设一些估计值，比如数值型特征，选择其余数据的中位数或众数作为当前的估计值，然后，根据估计的数值，建立随机森林，把所有的数据放进随机森林里面跑一遍。记录每一组数据在决策树中一步一步分类的路径，然后来判断哪组数据和缺失数据路径最相似，引入一个相似度矩阵，来记录数据之间的相似度，比如有N组数据，相似度矩阵大小就是N*N，如果缺失值是类别变量，通过权重投票得到新估计值，如果是数值型变量，通过加权平均得到新的估计值，如此迭代，直到得到稳定的估计值。

其实，该缺失值填补过程类似于推荐系统中采用协同过滤进行评分预测，先计算缺失特征与其他特征的相似度，再加权得到缺失值的估计，而随机森林中计算相似度的方法（数据在决策树中一步一步分类的路径）乃其独特之处。



## 简述K-Means

k-means是用来解决著名的聚类问题的最简单的非监督学习算法之一。
该过程遵循一个简易的方式，将一组数据划分为预先设定好的k个簇。其主要思想是为每个簇定义一个质心。设置这些质心需要一些技巧，因为不同的位置会产生不同的聚类结果。因此，较好的选择是使它们互相之间尽可能远。

接下来将数据中的每个点归类为距它最近的质心，距离的计算可以是欧式距离、曼哈顿距离、切比雪夫距离等。如果所有的数据点都归类完毕，那么第一步就结束了，早期的聚合过程也相应完成。

此时，我们根据上一步所产生的结果重新计算k个质心作为各个簇的质心。一旦获得k个新的质心，我们需要重新将数据集中的点与距它最近的新质心进行绑定。一个循环就此产生。作为循环的结果，我们发现k个质心逐步改变它们的位置，直至位置不再发生变化为止。
k-means算法是数值的、非监督的、非确定的、迭代的。

算法流程

1、从所有的观测实例中随机取出k个观测点，作为聚类的中心点；然后遍历奇遇的观测点找到各自距离最近的聚类中心点，并将其加入该聚类中。这样，我们便有了一个初始的聚类结果，这是一次迭代过程。
2、每个聚类中心都至少有一个观测实例，这样，我们便可以求出每个聚类的中心点，作为新的聚类中心（该聚类中所有实例的均值），然后再遍历其他所有的观测点，找到距离其最近的中心点，并加入到该聚类中。
3、如此重复步骤2，直到前后两次迭代得到的聚类中心点不再发生变化为止。

该算法旨在最小化一个目标函数——误差平方函数（所有的观测点与其中心点的距离之和）。





## 请说说Kmeans的优化？

k-means：在大数据的条件下，会耗费大量的时间和内存。 

  优化k-means的建议： 
  1、减少聚类的数目K。因为，每个样本都要跟类中心计算距离。 
  2、减少样本的特征维度。比如说，通过PCA等进行降维。 
  3、考察其他的聚类算法，通过选取toy数据，去测试不同聚类算法的性能。 
  4、hadoop集群，K-means算法是很容易进行并行计算的。 



## 什么是AUC？

AUC是一个模型评价指标，只能用于二分类模型的评价，对于二分类模型，还有很多其他评价指标，比如logloss，accuracy，precision。如果你经常关注数据挖掘比赛，比如kaggle，那你会发现AUC和logloss基本是最常见的模型评价指标。

为什么AUC和logloss比accuracy更常用呢？因为很多机器学习的模型对分类问题的预测结果都是概率，如果要计算accuracy，需要先把概率转化成类别，这就需要手动设置一个阈值，如果对一个样本的预测概率高于这个预测，就把这个样本放进一个类别里面，低于这个阈值，放进另一个类别里面。所以这个阈值很大程度上影响了accuracy的计算。使用AUC或者logloss可以避免把预测概率转换成类别。

AUC是Area under curve的首字母缩写。Area under curve是什么呢，从字面理解，就是一条曲线下面区域的面积。所以我们要先来弄清楚这条曲线是什么。这个曲线有个名字，叫ROC曲线。

ROC曲线是统计里面的概率，最早由电子工程师在二战中提出来（更多关于ROC的资料可以参考维基百科）。
ROC曲线是基于样本的真实类别和预测概率来画的，具体来说，ROC曲线的x轴是伪阳性率（false positive rate），y轴是真阳性率（true positive rate)。

那么问题来了，什么是真、伪阳性率呢？对于二分类问题，一个样本的类别只有两种，我们用0,1分别表示两种类别，0和1也可以分别叫做阴性和阳性。当我们用一个分类器进行概率的预测的时候，对于真实为0的样本，我们可能预测其为0或1，同样对于真实为1的样本，我们也可能预测其为0或1，这样就有四种可能性：

真阳性率=（真阳性的数量）/（真阳性的数量+伪阴性的数量）
伪阳性率=（伪阳性的数量）/（伪阳性的数量+真阴性的数量）

从Mann–Whitney U statistic的角度来解释，AUC就是从所有1样本中随机选取一个样本， 从所有0样本中随机选取一个样本，然后根据你的分类器对两个随机样本进行预测，把1样本预测为1的概率为p1，把0样本预测为1的概率为p0，p1>p0的概率就等于AUC。

所以AUC反应的是分类器对样本的排序能力。根据这个解释，如果我们完全随机的对样本分类，那么AUC应该接近0.5。另外值得注意的是，AUC对样本类别是否均衡并不敏感，这也是不均衡样本通常用AUC评价分类器性能的一个原因。



## 衡量分类器的好坏？

* 精度（Accuracy）

  ![img](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64159584020284145236.png)

* Recall：被正确预测的正例在所有正例中的比例，表示的是模型识别正例的能力。

  ![img](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64159584030351367352.png)

  Precision：表示被正确预测的正例在所有预测为正例的样本的比例。如果模型的查准率非常高，那么如果一个样本被预测为正例，那么这个样本为正例的把握就很大。

  ![img](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64159584032553206264.png)

  F score：就是一个将查全率和查准率结合起来的指标，能表达出对查准率/查全率的不同偏好。

  ![img](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64159584044313509254.png)

  ![img](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64159584055019638179.png)

  ![img](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64159584064198601945.png)

  ![img](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64159584062818019862.png)

  ROC曲线：ROC空间是一个以伪阳性率（FPR，false positive rate）为X轴，真阳性率（TPR, true positive rate）为Y轴的二维坐标系所代表的平面。其中真阳率TPR = TP / P = recall， 伪阳率FPR = FP / N

  

## 数据预处理

1. 缺失值，填充缺失值fillna：
i. 离散：None,
ii. 连续：均值。
iii. 缺失值太多，则直接去除该列
2. 连续值：离散化。有的模型（如决策树）需要离散值
3. 对定量特征二值化。核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0。如图像操作
4. 皮尔逊相关系数，去除高度相关的列



## 你知道有哪些数据处理和特征工程的处理？

![img](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com//Public/Image/Question/1512980743_407.png)



## 数据不平衡问题

采样，对小样本加噪声采样，对大样本进行下采样
数据生成，利用已知样本生成新的样本
进行特殊的加权，如在Adaboost中或者SVM中
采用对不平衡数据集不敏感的算法
改变评价标准：用AUC/ROC来进行评价
采用Bagging/Boosting/ensemble等方法
在设计模型的时候考虑数据的先验分布



1. 从数据角度
主动获取：获取更多的少量样本数据

针对少量样本数据，可以尽可能去扩大这些少量样本的数据集，或者尽可能去增加他们特有的特征来丰富数据的多样性。譬如，如果是一个情感分析项目，在分析数据比例时发现负样本（消极情感）的样本数量较少，那么我们可以尽可能在网站中搜集更多的负样本数量。

算法采样：上采样、下采样、生成合成数据

ADASYN采样方法：

ADASYN为样本较少的类生成合成数据，其生成的数据与更容易学习的样本相比，更难学习。基本思想是根据学习难度的不同，对不同的少数类的样本使用加权分布。其中，更难学习的少数类的样本比那些更容易学习的少数类的样本要产生更多的合成数据。因此，ADASYN方法通过以下两种方式改善了数据分布的学习：(1)减少由于类别不平衡带来的偏差；(2)自适应地将分类决策边界转移到困难的例子。

SMOTE采样方法:

从少数类创建新的合成点，以增加其基数。但是SMOTE算法也有一定的局限性。具体有两项，一是在近邻选择时，存在一定的盲目性。在算法执行过程中，需要确定Ｋ值，即选择几个近邻样本，这个需要根据具体的实验数据和实验人自己解决。二是该算法无法克服非平衡数据集的数据分布问题，容易产生分布边缘化的问题。由于负类样本的分布决定了其可选择的近邻，如果一个负类样本处在负类样本的边缘，则由此负类样本和近邻样本产生的样本也会处在边缘，从而无法确定正负类的分类边界。下图是以前做的一个项目应用个各种采样方法做数据增强的情况。（效果不明显，因为原始数据的分布重合太明显，可视化不容易显示出效果）

数据增强：加噪音增强模型鲁棒性、对不同性质的数据也可以做不同的augmentation

改变权重：设定惩罚因子，如libsvm等算法里设置的正负样本的权重项等。惩罚多样本类别，其实还可以加权少样本类别 

　　注意：在选择采样法事需要注意一个问题，如果你的实际数据是数据不平衡的，在训练模型时发现效果不好，于是采取了采样法平衡的数据的比例再来进行训练，然后去测试数据上预测，这个时候算法的效果是否会有偏差呢？此时你的训练样本的分布与测试样本的分布已经发生了改变，这样做反而会产生不好的效果。在实际情况中，我们尽可能的需要保持训练和测试的样本的概率分布是一致的，如果测试样本的分布是不平衡的，那么训练样本尽可能与测试样本的分布保持一致，哪怕拿到手的是已经清洗和做过预处理后的平衡的数据。具体原因感兴趣的可以仔细思考一下。

2.从评价指标角度
谨慎选择AUC作为评价指标：对于数据极端不平衡时，可以观察观察不同算法在同一份数据下的训练结果的precision和recall，这样做有两个好处，一是可以了解不同算法对于数据的敏感程度，二是可以明确采取哪种评价指标更合适。针对机器学习中的数据不平衡问题，建议更多PR(Precision-Recall曲线)，而非ROC曲线，具体原因画图即可得知，如果采用ROC曲线来作为评价指标，很容易因为AUC值高而忽略实际对少两样本的效果其实并不理想的情况。

不要只看Accuracy：Accuracy可以说是最模糊的一个指标了，因为这个指标高可能压根就不能代表业务的效果好，在实际生产中，我们可能更关注precision/recall/mAP等具体的指标，具体侧重那个指标，得结合实际情况看。

3.从算法角度
选择对数据倾斜相对不敏感的算法。如树模型等。
原因：
任何通过计算样本的Loss然后反向传播更新权值的算法，都会对正负样本的数量是否平衡很敏感。哪一类样本多，权重调整时就更倾向于调整到使该类样本错分率更低。不管是逻辑回归，还是深度学习。树模型不属于上一类，它是通过计算信息熵来选择特征划分样本的，划分后熵增最大。这个计算过程对于样本的数量的多少是不敏感的，因为熵的计算和样本数量无直接关系。此处可参考：https://www.zhihu.com/question/344124001。

集成学习（Ensemble集成算法）。首先从多数类中独立随机抽取出若干子集，将每个子集与少数类数据联合起来训练生成多个基分类器，再加权组成新的分类器，如加法模型、Adaboost、随机森林等。

将任务转换成异常检测问题。譬如有这样一个项目，需要从高压线的航拍图片中，将松动的螺丝/零件判断为待检测站点，即负样本，其他作为正样本，这样来看，数据倾斜是非常严重的，而且在图像质量一般的情况下小物体检测的难度较大，所以不如将其转换为无监督的异常检测算法，不用过多的去考虑将数据转换为平衡问题来解决。



## 常见的分类算法有哪些？他们各自的优缺点是什么？

贝叶斯分类法
优点：
1）所需估计的参数少，对于缺失数据不敏感。
2）有着坚实的数学基础，以及稳定的分类效率。	

缺点：
1）假设属性之间相互独立，这往往并不成立。（喜欢吃番茄、鸡蛋，却不喜欢吃番茄炒蛋）。
2）需要知道先验概率。
3）分类决策存在错误率。


决策树	
优点：
1）不需要任何领域知识或参数假设。
2）适合高维数据。
3）简单易于理解。
4）短时间内处理大量数据，得到可行且效果较好的结果。
5）能够同时处理数据型和常规性属性。	

缺点：
1）对于各类别样本数量不一致数据，信息增益偏向于那些具有更多数值的特征。
2）易于过拟合。
3）忽略属性之间的相关性。
4）不支持在线学习。


支持向量机	
优点：
1）可以解决小样本下机器学习的问题。
2）提高泛化性能。
3）可以解决高维、非线性问题。超高维文本分类仍受欢迎。
4）避免神经网络结构选择和局部极小的问题。	

缺点：
1）对缺失数据敏感。
2）内存消耗大，难以解释。
3）运行和调参略烦人。


K近邻	
优点：
1）思想简单，理论成熟，既可以用来做分类也可以用来做回归； 
2）可用于非线性分类； 
3）训练时间复杂度为O(n)； 
4）准确度高，对数据没有假设，对outlier不敏感；	

缺点：
1）计算量太大
2）对于样本分类不均衡的问题，会产生误判。
3）需要大量的内存。
4）输出的可解释性不强。


Logistic回归	
优点：
1）速度快。
2）简单易于理解，直接看到各个特征的权重。
3）能容易地更新模型吸收新的数据。
4）如果想要一个概率框架，动态调整分类阀值。	

缺点：
特征处理复杂。需要归一化和较多的特征工程。


神经网络	
优点：
1）分类准确率高。
2）并行处理能力强。
3）分布式存储和学习能力强。
4）鲁棒性较强，不易受噪声影响。	

缺点：
1）需要大量参数（网络拓扑、阀值、阈值）。
2）结果难以解释。
3）训练时间过长。


Adaboost
优点：
1）adaboost是一种有很高精度的分类器。
2）可以使用各种方法构建子分类器，Adaboost算法提供的是框架。
3）当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单。
4）简单，不用做特征筛选。
5）不用担心overfitting。	

缺点：
对outlier比较敏感





## 带核的SVM为什么能分类非线性问题？

核函数的本质是两个函数的內积，通过核函数将其隐射到高维空间，在高维空间非线性问题转化为线性问题, SVM得到超平面是高维空间的线性分类平面, 如图:

![img](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com//Public/Image/Question/1515085301_537.png)

 其分类结果也视为低维空间的非线性分类结果, 因而带核的SVM就能分类非线性问题。





## 请说说常用核函数及核函数的条件

我们通常说的核函数指的是正定和函数，其充要条件是对于任意的x属于X，要求K对应的Gram矩阵要是半正定矩阵。RBF核径向基，这类函数取值依赖于特定点间的距离，所以拉普拉斯核其实也是径向基核。SVM关键是选取核函数的类型，常用核函数主要有线性内核，多项式内核，径向基内核（RBF），sigmoid核。

线性核函数![img](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com//Public/Image/Question/1515085474_483.png)

线性核，主要用于线性可分的情况，我们可以看到特征空间到输入空间的维度是一样的，其参数少速度快，对于线性可分数据，其分类效果很理想，因此我们通常首先尝试用线性核函数来做分类，看看效果如何，如果不行再换别的

多项式核函数![img](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com//Public/Image/Question/1515085484_701.png)

多项式核函数可以实现将低维的输入空间映射到高纬的特征空间，但是多项式核函数的参数多，当多项式的阶数比较高的时候，核矩阵的元素值将趋于无穷大或者无穷小，计算复杂度会大到无法计算。

高斯（RBF）核函数![img](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com//Public/Image/Question/1515085498_834.png)

高斯径向基函数是一种局部性强的核函数，其可以将一个样本映射到一个更高维的空间内，该核函数是应用最广的一个，无论大样本还是小样本都有比较好的性能，而且其相对于多项式核函数参数要少，因此大多数情况下在不知道用什么核函数的时候，优先使用高斯核函数。

sigmoid核函数 ![img](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com//Public/Image/Question/1515085515_840.png)

采用sigmoid核函数，支持向量机实现的就是一种多层神经网络。

因此，在选用核函数的时候，如果我们对我们的数据有一定的先验知识，就利用先验来选择符合数据分布的核函数；如果不知道的话，通常使用交叉验证的方法，来试用不同的核函数，误差最下的即为效果最好的核函数，或者也可以将多个核函数结合起来，形成混合核函数。

在吴恩达的课上，也曾经给出过一系列的选择核函数的方法：
如果特征的数量大到和样本数量差不多，则选用LR或者线性核的SVM；
如果特征的数量小，样本的数量正常，则选用SVM+高斯核函数；
如果特征的数量小，而样本的数量很大，则需要手工添加一些特征从而变成第一种情况。



## 请具体说说Boosting和Bagging的区别

（1） Bagging之随机森林 
　　随机森林改变了决策树容易过拟合的问题，这主要是由两个操作所优化的：
　　1）Boostrap从袋内有放回的抽取样本值
　　2）每次随机抽取一定数量的特征（通常为sqr(n)）。 
　　分类问题：采用Bagging投票的方式选择类别频次最高的 
　　回归问题：直接取每颗树结果的平均值。
常见参数	误差分析	优点	缺点
1、树最大深度
2、树的个数 
3、节点上的最小样本数
4、特征数(sqr(n))	oob(out-of-bag)
将各个树的未采样样本作为预测样本统计误差作为误分率	可以并行计算
不需要特征选择
可以总结出特征重要性
可以处理缺失数据
不需要额外设计测试集	在回归上不能输出连续结果

（2）Boosting之AdaBoost 
　　Boosting的本质实际上是一个加法模型，通过改变训练样本权重学习多个分类器并进行一些线性组合。而Adaboost就是加法模型+指数损失函数+前项分布算法。Adaboost就是从弱分类器出发反复训练，在其中不断调整数据权重或者是概率分布，同时提高前一轮被弱分类器误分的样本的权值。最后用分类器进行投票表决（但是分类器的重要性不同）。 

（3）Boosting之GBDT 
　　将基分类器变成二叉树，回归用二叉回归树，分类用二叉分类树。和上面的Adaboost相比，回归树的损失函数为平方损失，同样可以用指数损失函数定义分类问题。但是对于一般损失函数怎么计算呢？GBDT（梯度提升决策树）是为了解决一般损失函数的优化问题，方法是用损失函数的负梯度在当前模型的值来模拟回归问题中残差的近似值。 
　　注：由于GBDT很容易出现过拟合的问题，所以推荐的GBDT深度不要超过6，而随机森林可以在15以上。 

（4）Boosting之Xgboost 
这个工具主要有以下几个特点：
支持线性分类器
可以自定义损失函数，并且可以用二阶偏导
加入了正则化项：叶节点数、每个叶节点输出score的L2-norm
支持特征抽样
在一定情况下支持并行，只有在建树的阶段才会用到，每个节点可以并行的寻找分裂特征。



## 对于维度极低的特征，选择线性还是非线性分类器？

非线性分类器，低维空间可能很多特征都跑到一起了，导致线性不可分。
1. 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM
2. 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel
3. 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况。



## 什么是偏差与方差？

泛化误差可以分解成偏差的平方加上方差加上噪声。偏差度量了学习算法的期望预测和真实结果的偏离程度，刻画了学习算法本身的拟合能力，方差度量了同样大小的训练集的变动所导致的学习性能的变化，刻画了数据扰动所造成的影响，噪声表达了当前任务上任何学习算法所能达到的期望泛化误差下界，刻画了问题本身的难度。偏差和方差一般称为bias和variance，一般训练程度越强，偏差越小，方差越大，泛化误差一般在中间有一个最小值，如果偏差较大，方差较小，此时一般称为欠拟合，而偏差较小，方差较大称为过拟合。

偏差：![img](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com//Public/Image/Question/1516721455_889.png)

方差：![img](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com//Public/Image/Question/1516721472_854.png)



## 解决bias和Variance问题的方法是什么？

High bias解决方案:Boosting、复杂模型（非线性模型、增加神经网络中的层）、更多特征
High Variance解决方案：bagging、简化模型、降维

具体而言
高偏差, 可以用boosting模型, 对预测残差进行优化, 直接降低了偏差. 也可以用高模型容量的复杂模型(比如非线性模型, 深度神经网络), 更多的特征, 来增加对样本的拟合度.
高方差, 一般使用平均值法, 比如bagging, 或者模型简化/降维方法, 来降低方差.

高偏差和高方差都是不好的, 我们应该加以避免. 但是它们又是此消彼长的关系, 所以必须权衡考虑. 一般情况下, 交叉验证训练可以取得比较好的平衡:
将原始样本均分成K组, 将每组样本分别做一次验证集,其余的K-1组子集数据作为训练集,这样会得到K个模型, 这K个模型可以并发训练以加速. 用这K个模型最终的验证集的分类准确率的平均数作为此K-CV下分类器的性能指标. K一般大于等于3, 而K-CV 的实验共需要建立 k 个models，并计算 k 次 test sets 的平均预测正确率。

在实作上，k 要够大才能使各回合中的 训练样本数够多，一般而言 k=10 (作为一个经验参数)算是相当足够了。



## 给你一个癌症检测的数据集。你已经建好了分类模型，取得了96％的精度。为什么你还是不满意你的模型性能？你可以做些什么呢？

答：如果你分析过足够多的数据集，你应该可以判断出来癌症检测结果是不平衡数据。在不平衡数据集中，精度不应该被用来作为衡量模型的标准，因为96％（按给定的）可能只有正确预测多数分类，但我们感兴趣是那些少数分类（4％），是那些被诊断出癌症的人。

因此，为了评价模型的性能，应该用灵敏度（真阳性率），特异性（真阴性率），F值用来确定这个分类器的“聪明”程度。如果在那4%的数据上表现不好，我们可以采取以下步骤：

1.我们可以使用欠采样、过采样或SMOTE让数据平衡。
2.我们可以通过概率验证和利用AUC-ROC曲线找到最佳阀值来调整预测阀值。
3.我们可以给分类分配权重，那样较少的分类获得较大的权重。
4.我们还可以使用异常检测。

注意：要更多地了解不平衡分类





## 简述PCA

PCA的思想是将n维特征映射到k维上（k < n），这k维是全新的正交特征。这k维特征称为主元，是重新构造出来的k维特征，而不是简单地从n维特征中去除其余n-k维特征。

计算过程

* 第一步分别求x和y的平均值，然后对于所有的样例，都减去对应的均值。

* 第二步，求特征协方差矩阵，如果数据是3维，那么协方差矩阵是

  ![img](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154918097069869010.png)

* 对角线上分别是x和y的方差，非对角线上是协方差。协方差大于0表示x和y若有一个增，另一个也增；小于0表示一个增，一个减；协方差为0时，两者独立。协方差绝对值越大，两者对彼此的影响越大，反之越小。

* 第三步，求协方差的特征值和特征向量，得到

*   第四步，将特征值按照从大到小的顺序排序，选择其中最大的k个，然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵。

* 第五步，将样本点投影到选取的特征向量上。假设样例数为m，特征数为n，减去均值后的样本矩阵为DataAdjust(m*n)，协方差矩阵是n*n，选取的k个特征向量组成的矩阵为EigenVectors(n*k)。那么投影后的数据FinalData为![img](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64154918100931452373.png)



PCA技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。

 PCA技术的一个很大的优点是，它是完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。 

但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。



## 决策树

https://zhuanlan.zhihu.com/p/32877396?refer=c_152307828

决策树是一种基本的分类与回归方法，其模型就是用一棵树来表示我们的整个决策过程。这棵树可以是二叉树（比如**CART 只能是二叉树**），也可以是多叉树（比如 ID3、C4.5 可以是多叉树或二叉树）。根节点包含整个样本集，每个叶节都对应一个决策结果（注意，不同的叶节点可能对应同一个决策结果），每一个内部节点都对应一次决策过程或者说是一次属性测试。从根节点到每个叶节点的路径对应一个判定测试序列。

决策树的生成就是不断的选择最优的特征对训练集进行划分，是一个递归的过程。递归返回的条件有三种：

1. 当前节点包含的样本属于同一类别，无需划分；
2. 当前属性集为空，或所有样本在属性集上取值相同，无法划分；
3. 当前节点包含样本集合为空，无法划分。

这三个是非常著名的决策树算法。简单粗暴来说:

- ID3 使用信息增益作为选择特征的准则；
- C4.5 使用信息增益比作为选择特征的准则；
- CART 使用 Gini 指数作为选择特征的准则。

* ID3

  熵表示的是数据中包含的信息量大小。熵越小，数据的纯度越高，也就是说数据越趋于一致，这是我们希望的划分之后每个子节点的样子。

  **信息增益 = 划分前熵 - 划分后熵**。信息增益越大，则意味着使用属性a来进行划分所获得的 “纯度提升” 越大 。也就是说，用属性a来划分训练集，得到的结果中纯度比较高。

  > ID3优点是理论清晰、方法简单、学习能力较强，但也存在一些缺点：

  - 只能处理分类属性的数据，不能处理连续的数据；
  - 划分过程会由于子集规模过小而造成统计特征不充分而停止；
  - ID3算法在选择根节点和各内部节点中的分支属性时，采用信息增益作为评价标准。信息增益的缺点是倾向于选择取值较多的属性，在有些情况下这类属性可能不会提供太多有价值的信息。

* **C4.5**

  C4.5 克服了 ID3 仅仅能够处理离散属性的问题，以及信息增益偏向选择取值较多特征的问题，使用信息增益比来选择特征。**信息增益比 = 信息增益 / 划分前熵** 选择信息增益比最大的作为最优特征。公式：

  ![[公式]](https://www.zhihu.com/equation?tex=g_R%28D%2CA%29%3D%5Cfrac%7Bg%28D%2CA%29%7D%7BH_A%28D%29%7D)

  其中 ![[公式]](https://www.zhihu.com/equation?tex=H_A%28D%29%3D-%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7B%5Cfrac%7B%7CD_i%7C%7D%7B%7CD%7C%7Dlog_2%5Cfrac%7B%7CD_i%7C%7D%7B%7CD%7C%7D%7D) ， ![[公式]](https://www.zhihu.com/equation?tex=n) 是特征 ![[公式]](https://www.zhihu.com/equation?tex=A) 取值的个数

  C4.5 处理连续特征是先将特征取值排序，以连续两个值中间值作为划分标准。尝试每一种划分，并计算修正后的信息增益，选择信息增益最大的分裂点作为该属性的分裂点。

* **CART**

  CART 与 ID3，C4.5 不同之处在于 CART 生成的树必须是二叉树。也就是说，无论是回归还是分类问题，无论特征是离散的还是连续的，无论属性取值有多个还是两个，内部节点只能根据属性值进行二分。

  CART 的全称是分类与回归树（classification and regression tree）。从这个名字中就应该知道，CART 既可以用于分类问题，也可以用于回归问题。



# 2. 大数据

## 简答说一下hadoop的map-reduce编程模型

首先map task会从本地文件系统读取数据，转换成key-value形式的键值对集合

使用的是hadoop内置的数据类型，比如longwritable、text等

将键值对集合输入mapper进行业务处理过程，将其转换成需要的key-value在输出

之后会进行一个partition分区操作，默认使用的是hashpartitioner，可以通过重写hashpartitioner的getpartition方法来自定义分区规则

之后会对key进行进行sort排序，grouping分组操作将相同key的value合并分组输出，在这里可以使用自定义的数据类型，重写WritableComparator的Comparator方法来自定义排序规则，重写RawComparator的compara方法来自定义分组规则

之后进行一个combiner归约操作，其实就是一个本地段的reduce预处理，以减小后面shufle和reducer的工作量

reduce task会通过网络将各个数据收集进行reduce处理，最后将数据保存或者显示，结束整个job



## hadoop和spark的都是并行计算，那么他们有什么相同和区别

两者都是用mr模型来进行并行计算，hadoop的一个作业称为job，job里面分为map task和reduce task，每个task都是在自己的进程中运行的，当task结束时，进程也会结束

spark用户提交的任务成为application，一个application对应一个sparkcontext，app中存在多个job，每触发一次action操作就会产生一个job

这些job可以并行或串行执行，每个job中有多个stage，stage是shuffle过程中DAGSchaduler通过RDD之间的依赖关系划分job而来的，每个stage里面有多个task，组成taskset有TaskSchaduler分发到各个executor中执行，executor的生命周期是和app一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算

hadoop的job只有map和reduce操作，表达能力比较欠缺而且在mr过程中会重复的读写hdfs，造成大量的io操作，多个job需要自己管理关系

spark的迭代计算都是在内存中进行的，API中提供了大量的RDD操作如join，groupby等，而且通过DAG图可以实现良好的容错









# 3.目标检测

## 3.1 原理

### 1）讲一下One-stage和Two-stage两种的区别？

* One-stage: 直接输出各个物体的类别概率和位置坐标信息，Yolo，SSD，CenterNet
* Two-Stage：分为两个阶段，第一阶段，在原图上产生大量的目标候选区域，其中包含目标的位置信息，第二阶段再对候选区域进行分类和位置的精细化调整  RCNN系列



### 2） Yolo迭代史

https://www.huaweicloud.com/articles/1bbc978269862843879e580761883927.html

 