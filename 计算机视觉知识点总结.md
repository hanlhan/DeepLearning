# 计算机视觉知识点总结

> 来源： [https://zhuanlan.zhihu.com/p/58776542](https://zhuanlan.zhihu.com/p/58776542)

最近会细化专栏的内容，以后有关计算机视觉的文章会分享在计算机视觉专栏上，欢迎有兴趣的朋友关注

[计算机视觉​](https://zhuanlan.zhihu.com/c_1093832350336720896)

[toc]


- 0 计算机视觉四大基本任务
- 1 经典卷积网络
- 2 卷积、空洞卷积
- 3 正则化
- 4 全卷积网络
- 5 1*1卷积核
- 6 感受野
- 7 常见损失
- 8 优化算法
- 9 concat 和 add的区别
- 10 注意力机制
- 11 CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)
- 12 边框回归
- 13 非极大值回归(NMS, Soft NMS)
- 14 激活函数
- 15 评价指标
- 16 batch size的选择
- 17 Graph Convolutional Network（GCN）
- 18 详解深度学习中的梯度消失、爆炸原因及其解决方法
- 19 网络权重初始化
- 其他

---

## 0 计算机视觉四大基本任务

[张皓：(二)计算机视觉四大基本任务(分类、定位、检测、分割)1427 赞同 · 99 评论文章](https://zhuanlan.zhihu.com/p/31727402)

![https://pic4.zhimg.com/v2-289b6c3133248f83626ff2d27ae44013_180x120.jpg](https://pic4.zhimg.com/v2-289b6c3133248f83626ff2d27ae44013_180x120.jpg)

- 目标检测解读汇总

[JustDoIT：目标检测论文及解读汇总64 赞同 · 3 评论文章](https://zhuanlan.zhihu.com/p/36402362)

![https://pic4.zhimg.com/v2-c6b449675274b7492d05db19f73fba03_180x120.jpg](https://pic4.zhimg.com/v2-c6b449675274b7492d05db19f73fba03_180x120.jpg)

- 图像语义分割综述

[stone：图像语义分割综述487 赞同 · 15 评论文章](https://zhuanlan.zhihu.com/p/37801090)

![https://pic1.zhimg.com/v2-464813a7c07f9e03c9d3db1ddc632f14_180x120.jpg](https://pic1.zhimg.com/v2-464813a7c07f9e03c9d3db1ddc632f14_180x120.jpg)

## 1 经典卷积网络

[SIGAI：深度卷积神经网络演化历史及结构改进脉络-40页长文全面解读297 赞同 · 35 评论文章](https://zhuanlan.zhihu.com/p/36765623)

![https://pic4.zhimg.com/v2-fb77b78af9a4f83de66a6f855c0d9d0b_180x120.jpg](https://pic4.zhimg.com/v2-fb77b78af9a4f83de66a6f855c0d9d0b_180x120.jpg)

- AlexNet

[从AlexNet理解卷积神经网络的一般结构​blog.csdn.net/chaipp0607/article/details/72847422](https://link.zhihu.com/?target=https%3A//blog.csdn.net/chaipp0607/article/details/72847422)

- VGG

[【深度学习】经典神经网络 VGG 论文解读​blog.csdn.net/briblue/article/details/83792394](https://link.zhihu.com/?target=https%3A//blog.csdn.net/briblue/article/details/83792394)

[深度学习之基础模型-VGG​blog.csdn.net/whz1861/article/details/78111606](https://link.zhihu.com/?target=https%3A//blog.csdn.net/whz1861/article/details/78111606)

- GoogleNet

[深度学习卷积神经网络–经典网络GoogLeNet(Inception V3)网络的搭建与实现​blog.csdn.net/loveliuzz/article/details/79135583](https://link.zhihu.com/?target=https%3A//blog.csdn.net/loveliuzz/article/details/79135583)

- ResNet

[ResNet解析 - lanran2的博客 - CSDN博客​blog.csdn.net/lanran2/article/details/79057994](https://link.zhihu.com/?target=https%3A//blog.csdn.net/lanran2/article/details/79057994)

[ResNetV2：ResNet深度解析 - lanran2的博客 - CSDN博客​blog.csdn.net/lanran2/article/details/80247515](https://link.zhihu.com/?target=https%3A//blog.csdn.net/lanran2/article/details/80247515)

[Resnet到底在解决一个什么问题呢？2084 赞同 · 105 评论回答](https://www.zhihu.com/question/64494691/answer/786270699)

![https://pic4.zhimg.com/v2-e0e43e18c61a82e24e5a837740843963_180x120.jpg](https://pic4.zhimg.com/v2-e0e43e18c61a82e24e5a837740843963_180x120.jpg)

- Xception

[Xception算法详解​blog.csdn.net/u014380165/article/details/75142710](https://link.zhihu.com/?target=https%3A//blog.csdn.net/u014380165/article/details/75142710)

- DenseNet

[DenseNet算法详解​blog.csdn.net/u014380165/article/details/75142664/](https://link.zhihu.com/?target=https%3A//blog.csdn.net/u014380165/article/details/75142664/)

- DetNet

[旷视科技：旷视科技提出物体检测专用Backbone——DetNet216 赞同 · 7 评论文章](https://zhuanlan.zhihu.com/p/39702482)

![https://pic4.zhimg.com/v2-622d5e7abe3f01b332e372b8de1cc13f_180x120.jpg](https://pic4.zhimg.com/v2-622d5e7abe3f01b332e372b8de1cc13f_180x120.jpg)

- ShuffleNet

[小小将：ShuffleNetV2：轻量级CNN网络中的桂冠330 赞同 · 37 评论文章](https://zhuanlan.zhihu.com/p/48261931)

![https://pic1.zhimg.com/v2-fdca446266001fb14c93e1b92f3b2e88_180x120.jpg](https://pic1.zhimg.com/v2-fdca446266001fb14c93e1b92f3b2e88_180x120.jpg)

- HRNet

[pprp：打通多个视觉任务的全能Backbone:HRNet319 赞同 · 57 评论文章](https://zhuanlan.zhihu.com/p/134253318)

![https://pic2.zhimg.com/v2-7d109af3d7488964abdf9b5345133501_180x120.jpg](https://pic2.zhimg.com/v2-7d109af3d7488964abdf9b5345133501_180x120.jpg)

## 2 卷积、空洞卷积

[Justin ho：变形卷积核、可分离卷积？卷积神经网络中十大拍案叫绝的操作。2786 赞同 · 67 评论文章](https://zhuanlan.zhihu.com/p/28749411)

![https://pic4.zhimg.com/v2-4f916a0b75eb237872c428bae001e4ef_180x120.jpg](https://pic4.zhimg.com/v2-4f916a0b75eb237872c428bae001e4ef_180x120.jpg)

- 卷积

[如何通俗易懂地解释卷积？1.2 万关注 · 236 回答问题](https://www.zhihu.com/question/22298352)

[“看懂”卷积神经网(Visualizing and Understanding Convolutional Networks)​blog.csdn.net/kklots/article/details/17136059](https://link.zhihu.com/?target=https%3A//blog.csdn.net/kklots/article/details/17136059)

[CNN入门必读经典：Visualizing and Understanding Convolutional Networks​blog.csdn.net/bea_tree/article/details/68954650](https://link.zhihu.com/?target=https%3A//blog.csdn.net/bea_tree/article/details/68954650)

[蒋竺波：CNN入门讲解：卷积层是如何提取特征的？876 赞同 · 102 评论文章](https://zhuanlan.zhihu.com/p/31657315)

![https://pic1.zhimg.com/v2-b90b2a6b37b92e34ac4f8dea58286630_180x120.jpg](https://pic1.zhimg.com/v2-b90b2a6b37b92e34ac4f8dea58286630_180x120.jpg)

- 空洞卷积

[如何理解空洞卷积（dilated convolution）？1619 赞同 · 74 评论回答](https://www.zhihu.com/question/54149221/answer/323880412)

![https://pic2.zhimg.com/v2-b2b6f12a4c3d244c4bc7eb33814a1f0d_180x120.jpg](https://pic2.zhimg.com/v2-b2b6f12a4c3d244c4bc7eb33814a1f0d_180x120.jpg)

[yyfyan：总结-空洞卷积(Dilated/Atrous Convolution)536 赞同 · 54 评论文章](https://zhuanlan.zhihu.com/p/50369448)

![https://pic2.zhimg.com/v2-ecd543f79acf88f965a505772b86a40d_180x120.jpg](https://pic2.zhimg.com/v2-ecd543f79acf88f965a505772b86a40d_180x120.jpg)

- 可形变卷积

[如何评价 MSRA 最新的 Deformable Convolutional Networks？318 赞同 · 6 评论回答](https://www.zhihu.com/question/57493889/answer/184578752)

![https://pic3.zhimg.com/v2-9d28f60a0566871bff39f146d88d946e_180x120.jpg](https://pic3.zhimg.com/v2-9d28f60a0566871bff39f146d88d946e_180x120.jpg)

## 3 正则化

- batch normalization、group normalization

[Batch Normalization导读​blog.csdn.net/malefactor/article/details/51476961](https://link.zhihu.com/?target=https%3A//blog.csdn.net/malefactor/article/details/51476961)

[张俊林：深度学习中的Normalization模型1139 赞同 · 71 评论文章](https://zhuanlan.zhihu.com/p/43200897)

![https://pic2.zhimg.com/v2-d9950c280fda77a744d0487e3f8baf25_180x120.jpg](https://pic2.zhimg.com/v2-d9950c280fda77a744d0487e3f8baf25_180x120.jpg)

[Feng Nie：Group Normalization 及其MXNet、Gluon实现34 赞同 · 5 评论文章](https://zhuanlan.zhihu.com/p/56219719)

![https://pic1.zhimg.com/v2-63ec802e6d95094dd4d6f1cc0cc994ac_180x120.jpg](https://pic1.zhimg.com/v2-63ec802e6d95094dd4d6f1cc0cc994ac_180x120.jpg)

[Juliuszh：详解深度学习中的Normalization，BN/LN/WN3379 赞同 · 124 评论文章](https://zhuanlan.zhihu.com/p/33173246)

![https://pic2.zhimg.com/v2-67e63301b77923897960fb4b50a84ed9_180x120.jpg](https://pic2.zhimg.com/v2-67e63301b77923897960fb4b50a84ed9_180x120.jpg)

- dropout

[理解dropout - 雨石 - CSDN博客​blog.csdn.net/stdcoutzyx/article/details/49022443](https://link.zhihu.com/?target=https%3A//blog.csdn.net/stdcoutzyx/article/details/49022443)

- L1、L2

[机器学习中的范数规则化之（一）L0、L1与L2范数​blog.csdn.net/zouxy09/article/details/24971995](https://link.zhihu.com/?target=https%3A//blog.csdn.net/zouxy09/article/details/24971995)

[机器学习中正则化项L1和L2的直观理解​blog.csdn.net/jinping_shi/article/details/52433975](https://link.zhihu.com/?target=https%3A//blog.csdn.net/jinping_shi/article/details/52433975)

## 4 全卷积网络

[全卷积网络 FCN 详解 - 代码学习者coding - 博客园​www.cnblogs.com/gujianhan/p/6030639.html](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/gujianhan/p/6030639.html)

## 5 1*1卷积核

[Amusi：一文读懂卷积神经网络中的1x1卷积核905 赞同 · 67 评论文章](https://zhuanlan.zhihu.com/p/40050371)

![https://pic2.zhimg.com/v2-b84aead173acf729ba838eea526a6069_180x120.jpg](https://pic2.zhimg.com/v2-b84aead173acf729ba838eea526a6069_180x120.jpg)

[卷积神经网络中用1*1 卷积有什么作用或者好处呢？1284 关注 · 37 回答问题](https://www.zhihu.com/question/56024942)

[如何理解卷积神经网络中的1*1卷积​blog.csdn.net/chaipp0607/article/details/60868689](https://link.zhihu.com/?target=https%3A//blog.csdn.net/chaipp0607/article/details/60868689)

## 6 感受野

[蓝荣祎：深度神经网络中的感受野(Receptive Field)284 赞同 · 32 评论文章](https://zhuanlan.zhihu.com/p/28492837)

![https://pic4.zhimg.com/v2-19d758faaff34bd64b0fc2b8ac5f084b_180x120.jpg](https://pic4.zhimg.com/v2-19d758faaff34bd64b0fc2b8ac5f084b_180x120.jpg)

[小小将：你知道如何计算CNN感受野吗？这里有一份详细指南366 赞同 · 44 评论文章](https://zhuanlan.zhihu.com/p/35708466)

![https://pic2.zhimg.com/v2-bf2e71cf68c1f1af5921087c3f928781_180x120.jpg](https://pic2.zhimg.com/v2-bf2e71cf68c1f1af5921087c3f928781_180x120.jpg)

## 7 常见损失

[王桂波：机器学习常用损失函数小结563 赞同 · 36 评论文章](https://zhuanlan.zhihu.com/p/77686118)

![https://pic4.zhimg.com/v2-b9b7ca7bb07d8ac5771f7a05d52f0a27_180x120.jpg](https://pic4.zhimg.com/v2-b9b7ca7bb07d8ac5771f7a05d52f0a27_180x120.jpg)

[损失函数改进方法总览​blog.csdn.net/u014380165/article/details/76946358](https://link.zhihu.com/?target=https%3A//blog.csdn.net/u014380165/article/details/76946358)

[请问faster rcnn和ssd 中为什么用smooth l1 loss，和l2有什么区别？398 赞同 · 21 评论回答](https://www.zhihu.com/question/58200555/answer/621174180)

- focal loss

[张俊：何恺明大神的「Focal Loss」，如何更好地理解？833 赞同 · 33 评论文章](https://zhuanlan.zhihu.com/p/32423092)

![https://pic3.zhimg.com/v2-c9ae876753569b550dab0f86c07e8ede_180x120.jpg](https://pic3.zhimg.com/v2-c9ae876753569b550dab0f86c07e8ede_180x120.jpg)

- 交叉熵

[交叉熵损失函数 - William Zhao’s notes - CSDN博客​blog.csdn.net/yimingsilence/article/details/52740638](https://link.zhihu.com/?target=https%3A//blog.csdn.net/yimingsilence/article/details/52740638)

[卷积神经网络系列之softmax，softmax loss和cross entropy的讲解​blog.csdn.net/u014380165/article/details/77284921](https://link.zhihu.com/?target=https%3A//blog.csdn.net/u014380165/article/details/77284921)

- 对比损失（Contrastive Loss）

[Contrastive Loss (对比损失)​blog.csdn.net/autocyz/article/details/53149760](https://link.zhihu.com/?target=https%3A//blog.csdn.net/autocyz/article/details/53149760)

- 三元组损失（Triplet Loss）

[Triplet Loss及其梯度​blog.csdn.net/jcjx0315/article/details/77160273](https://link.zhihu.com/?target=https%3A//blog.csdn.net/jcjx0315/article/details/77160273)

## 8 优化算法

[Juliuszh：一个框架看懂优化算法之异同 SGD/AdaGrad/Adam3243 赞同 · 125 评论文章](https://zhuanlan.zhihu.com/p/32230623)

- Momentum

[冯超：路遥知马力——Momentum238 赞同 · 37 评论文章](https://zhuanlan.zhihu.com/p/21486826)

- Nesterov Accelerated Gradient

[郑华滨：比Momentum更快：揭开Nesterov Accelerated Gradient的真面目895 赞同 · 66 评论文章](https://zhuanlan.zhihu.com/p/22810533)

![https://pic4.zhimg.com/v2-e84be95e5d7d58e3b0c7c85df325606f_180x120.jpg](https://pic4.zhimg.com/v2-e84be95e5d7d58e3b0c7c85df325606f_180x120.jpg)

## 9 concat 和 add的区别

[如何理解神经网络中通过add的方式融合特征？695 赞同 · 40 评论回答](https://www.zhihu.com/question/306213462/answer/562776112)

![https://pic2.zhimg.com/v2-705b43e596d650e706a5d6c4c98832f1_ipico.jpg](https://pic2.zhimg.com/v2-705b43e596d650e706a5d6c4c98832f1_ipico.jpg)

## 10 注意力机制

[瑟木：计算机视觉中的注意力机制190 赞同 · 20 评论文章](https://zhuanlan.zhihu.com/p/32928645)

![https://pic3.zhimg.com/v2-3ed269820465caa7b59d6d2ee601f926_180x120.jpg](https://pic3.zhimg.com/v2-3ed269820465caa7b59d6d2ee601f926_180x120.jpg)

[张戎：计算机视觉中的注意力机制354 赞同 · 9 评论文章](https://zhuanlan.zhihu.com/p/56501461)

![https://pic3.zhimg.com/v2-db501cac982e27570e8dee8ad9e93582_180x120.jpg](https://pic3.zhimg.com/v2-db501cac982e27570e8dee8ad9e93582_180x120.jpg)

## 11 CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)

[CNN(卷积神经网络)、RNN(循环神经网络)、DNN(深度神经网络)的内部网络结构有什么区别？6042 赞同 · 132 评论回答](https://www.zhihu.com/question/34681168/answer/84061846)

## 12 边框回归

[边框回归(Bounding Box Regression)详解​blog.csdn.net/zijin0802034/article/details/77685438](https://link.zhihu.com/?target=https%3A//blog.csdn.net/zijin0802034/article/details/77685438)

## 13 非极大值回归(NMS, Soft NMS)

- NMS

[NMS——非极大值抑制​blog.csdn.net/shuzfan/article/details/52711706](https://link.zhihu.com/?target=https%3A//blog.csdn.net/shuzfan/article/details/52711706)

- Soft NMS

[Soft NMS算法笔记​blog.csdn.net/u014380165/article/details/79502197](https://link.zhihu.com/?target=https%3A//blog.csdn.net/u014380165/article/details/79502197)

## 14 激活函数

[程程：深度学习中的激活函数导引245 赞同 · 18 评论文章](https://zhuanlan.zhihu.com/p/22142013)

![https://pic1.zhimg.com/ffa57442611ce65a1665de8e844de768_180x120.jpg](https://pic1.zhimg.com/ffa57442611ce65a1665de8e844de768_180x120.jpg)

[如何理解ReLU activation function?70 赞同 · 14 评论回答](https://www.zhihu.com/question/59031444/answer/177786603)

[请问人工神经网络中的activation function的作用具体是什么？为什么ReLu要好过于tanh和sigmoid function?618 赞同 · 72 评论回答](https://www.zhihu.com/question/29021768/answer/43488153)

## 15 评价指标

- 目标检测mAP

[rafaelpadilla/Object-Detection-Metrics​github.com/rafaelpadilla/Object-Detection-Metrics](https://link.zhihu.com/?target=https%3A//github.com/rafaelpadilla/Object-Detection-Metrics)

![https://pic1.zhimg.com/v2-e3908e52da06e33a458adbbb7bc6c4cc_ipico.jpg](https://pic1.zhimg.com/v2-e3908e52da06e33a458adbbb7bc6c4cc_ipico.jpg)

- 语义分割(PA、MPA、MIoU、FWIoU)

[JustDoIT：语义分割之评价指标71 赞同 · 25 评论文章](https://zhuanlan.zhihu.com/p/61880018)

![https://pic2.zhimg.com/v2-f0a9c5f39358cfca939ae3a715c17919_180x120.jpg](https://pic2.zhimg.com/v2-f0a9c5f39358cfca939ae3a715c17919_180x120.jpg)

可参考下面论文第五章节

[A Review on Deep Learning Techniques Applied to Semantic Segmentation​arxiv.org/abs/1704.06857](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1704.06857)

## 16 batch size的选择

[怎么选取训练神经网络时的Batch size?1037 赞同 · 55 评论回答](https://www.zhihu.com/question/61607442/answer/440401209)

![https://pic4.zhimg.com/v2-c0fef2aef59d5a9ef84aefbc7d74ed6f_ipico.jpg](https://pic4.zhimg.com/v2-c0fef2aef59d5a9ef84aefbc7d74ed6f_ipico.jpg)

[深度机器学习中的batch的大小对学习效果有何影响？2151 赞同 · 57 评论回答](https://www.zhihu.com/question/32673260/answer/71137399)

![https://pic2.zhimg.com/f5a6d3b5c4b5a91851f0f8b8735f162d_180x120.jpg](https://pic2.zhimg.com/f5a6d3b5c4b5a91851f0f8b8735f162d_180x120.jpg)

## 17 Graph Convolutional Network（GCN）

[如何理解 Graph Convolutional Network（GCN）？8539 赞同 · 781 评论回答](https://www.zhihu.com/question/54504471/answer/332657604)

![https://pic4.zhimg.com/v2-394cb7b5f6dfb23dcddd838ebdee548b_180x120.jpg](https://pic4.zhimg.com/v2-394cb7b5f6dfb23dcddd838ebdee548b_180x120.jpg)

## 18 详解深度学习中的梯度消失、爆炸原因及其解决方法

[DoubleV：详解深度学习中的梯度消失、爆炸原因及其解决方法798 赞同 · 54 评论文章](https://zhuanlan.zhihu.com/p/33006526)

![https://pic4.zhimg.com/v2-dffdfc852ee891e6f11ae068efa5737f_180x120.jpg](https://pic4.zhimg.com/v2-dffdfc852ee891e6f11ae068efa5737f_180x120.jpg)

## 19 网络权重初始化

[网络权重初始化方法总结（上）：梯度消失、梯度爆炸与不良的初始化 - shine-lee - 博客园​www.cnblogs.com/shine-lee/p/11809979.html](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/shine-lee/p/11809979.html)

![https://pic1.zhimg.com/v2-7271ed4c3ed2d47f1a577c63c3543558_180x120.jpg](https://pic1.zhimg.com/v2-7271ed4c3ed2d47f1a577c63c3543558_180x120.jpg)

[https://www.cnblogs.com/shine-lee/p/11908610.html​www.cnblogs.com/shine-lee/p/11908610.html](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/shine-lee/p/11908610.html)

## 其他

[卷积神经网络(CNN)反向传播算法 - 刘建平Pinard - 博客园​www.cnblogs.com/pinard/p/6494810.html](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pinard/p/6494810.html)

[神经网络中 warmup 策略为什么有效；有什么理论解释么？1148 赞同 · 17 评论回答](https://www.zhihu.com/question/338066667/answer/771252708)

![https://pic2.zhimg.com/v2-c9216211238aa8a2a668f049987a9919_180x120.jpg](https://pic2.zhimg.com/v2-c9216211238aa8a2a668f049987a9919_180x120.jpg)
